{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses PyDriller 1.9.2 (https://pypi.org/project/PyDriller/) to mine file history from gitHub repos.\n",
    "\n",
    "This notebook also calls a script ('affected.py') in a local copy of the oceanobservatories/preload database (https://github.com/oceanobservatories/preload-database).  Note, this script has dependancies on the ooi-data repo as well (https://github.com/oceanobservatories/ooi-data).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydriller import RepositoryMining\n",
    "import re\n",
    "from subprocess import Popen, PIPE\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "import csv\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters will need to be configured for MIO-specific definitions.  (Also note, some regEx strings in the code depend on a directory structure similar to '/Users/rsn/asset-management/' and a filename structure similar to \"ATAPL-12345-00001__20190101.csv\" and may need to be modified to match other local directory structures.)\n",
    "\n",
    "localRepo: local directory for asset management gitHub repo\n",
    "\n",
    "localDir: local directory for other repos\n",
    "\n",
    "sensorList: list of MIO-specific sensors with cal files on gitHub\n",
    "\n",
    "filePrefix: prefix of cal files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up file pathways\n",
    "\n",
    "localRepo = ('/Users/rsn/asset-management/')\n",
    "localDir = ('/Users/rsn/')\n",
    "\n",
    "sensorList = ['CTDPFA','CTDBPO','CTDBPN','CTDPFB','DOFSTA','FLCDRA','FLNTUA','SPKIRA', \\\n",
    "              'NUTNRA', 'OPTAAC', 'OPTAAD', 'PARADA', 'DOSTAD', 'FLORDD', 'PCO2WA', 'PCO2WB', \\\n",
    "              'PHSENA', 'PHSEND', 'THSPHA', 'TMPSFA', 'TRHPHA', 'VEL3DA', 'ZPLSCB']\n",
    "filePrefix = 'AT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commitMergeDate(commitHash):\n",
    "    ### return the merge date of a single commit given the commit hash\n",
    "    \n",
    "    mergeDate = -99999\n",
    "    \n",
    "    git_command = ['git', 'when-merged','-c', commitHash]\n",
    "    repository  = path.dirname(localRepo) \n",
    "\n",
    "    git_query = Popen(git_command, cwd=repository, stdout=PIPE, stderr=PIPE)\n",
    "    (git_status, error) = git_query.communicate()\n",
    "    if git_status:\n",
    "        mergeHash = re.search(r\"([a-zA-Z0-9]{40})\",str(git_status)).group(1)\n",
    "        if mergeHash:\n",
    "            for commit in RepositoryMining(localRepo, single=mergeHash).traverse_commits():\n",
    "                mergeDate = commit.author_date.strftime(\"%Y%m%dT%H%M%S\")\n",
    "    elif 'Commit is directly on this branch' in str(error):\n",
    "        mergeDate = 'directCommit'\n",
    "\n",
    "    return mergeDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gitHubMine(path_to_repo,path_to_fileName, fileName):\n",
    "    ### use the pyDriller \n",
    "\n",
    "    rename=[]\n",
    "    gitHubHistory={}\n",
    "    commitHash=[]\n",
    "    mods = ['ADD','MODIFY','DELETE','COPY','RENAME']\n",
    "\n",
    "    for commit in RepositoryMining(path_to_repo, filepath=path_to_fileName+fileName).traverse_commits():\n",
    "        for mod in commit.modifications:\n",
    "            if fileName in mod.filename:\n",
    "                if any(modType in str(mod.change_type) for modType in mods):\n",
    "                    commitHash.append(commit.hash)\n",
    "                    mergeDate = commitMergeDate(commit.hash)\n",
    "                    print(mergeDate)\n",
    "                    if 'directCommit' in str(mergeDate):\n",
    "                        mergeDate = commit.author_date.strftime(\"%Y%m%dT%H%M%S\")\n",
    "                    elif '-99999' in str(mergeDate):\n",
    "                        mergeDate = 'unknown'\n",
    "                    if mergeDate not in gitHubHistory:\n",
    "                        gitHubHistory[mergeDate]={}  \n",
    "                    gitHubHistory[mergeDate][commit.author_date.strftime(\"%Y%m%dT%H%M%S\")]  = {'commit': commit.hash, 'modType': str(mod.change_type), 'fileName': mod.new_path, 'oldFileName': mod.old_path}    \n",
    "                    if 'RENAME' in str(mod.change_type):\n",
    "                        oldFileName = re.search(r\"calibration\\/.*\\/(.*)\", mod.old_path).group(1)\n",
    "                        if oldFileName:\n",
    "                            rename.append(oldFileName)\n",
    "                    if 'MODIFY' in str(mod.change_type):\n",
    "                        modCategory = list(categorizeCalModification(path_to_repo, commit.hash, mod.new_path))\n",
    "                        if not modCategory:\n",
    "                            modCategory = ['notClassified']\n",
    "                        gitHubHistory[mergeDate][commit.author_date.strftime(\"%Y%m%dT%H%M%S\")]['modCategory'] = modCategory\n",
    "                    \n",
    "    if len(rename) > 0:\n",
    "        for renamedFile in rename:\n",
    "            for commit in RepositoryMining(path_to_repo, filepath=path_to_fileName+fileName).traverse_commits():\n",
    "                for mod in commit.modifications:\n",
    "                    if renamedFile in mod.filename:\n",
    "                        if any(modType in str(mod.change_type) for modType in mods):\n",
    "                            if commit.hash not in commitHash:\n",
    "                                commitHash.append(commit.hash)\n",
    "                                mergeDate = commitMergeDate(commit.hash)\n",
    "                                if 'directCommit' in mergeDate:\n",
    "                                    mergeDate = commit.author_date.strftime(\"%Y%m%dT%H%M%S\")\n",
    "                                elif '-99999' in mergeDate:\n",
    "                                    mergeDate = 'unknown'\n",
    "                                if mergeDate not in gitHubHistory:\n",
    "                                    gitHubHistory[mergeDate]={}\n",
    "                                gitHubHistory[mergeDate][commit.author_date.strftime(\"%Y%m%dT%H%M%S\")]  = {'commit': commit.hash, 'modType': str(mod.change_type), 'fileName': mod.new_path, 'oldFileName': mod.old_path}   \n",
    "                                if 'MODIFY' in str(mod.change_type):\n",
    "                                    modCategory = list(categorizeCalModification(path_to_repo, commit.hash, mod.new_path))\n",
    "                                    if not modCategory:\n",
    "                                        modCategory = ['notClassified']\n",
    "                                    gitHubHistory[mergeDate][commit.author_date.strftime(\"%Y%m%dT%H%M%S\")]['modCategory'] = modCategory\n",
    "                               \n",
    "    return(gitHubHistory)                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_sigfigs(num, sig_figs):\n",
    "    import math\n",
    "    \"\"\"Round to specified number of sigfigs.\n",
    "    from: http://code.activestate.com/recipes/578114-round-number-to-specified-number-of-significant-di/\n",
    "\n",
    "    >>> round_sigfigs(0, sig_figs=4)\n",
    "    0\n",
    "    >>> int(round_sigfigs(12345, sig_figs=2))\n",
    "    12000\n",
    "    >>> int(round_sigfigs(-12345, sig_figs=2))\n",
    "    -12000\n",
    "    >>> int(round_sigfigs(1, sig_figs=2))\n",
    "    1\n",
    "    >>> '{0:.3}'.format(round_sigfigs(3.1415, sig_figs=2))\n",
    "    '3.1'\n",
    "    >>> '{0:.3}'.format(round_sigfigs(-3.1415, sig_figs=2))\n",
    "    '-3.1'\n",
    "    >>> '{0:.5}'.format(round_sigfigs(0.00098765, sig_figs=2))\n",
    "    '0.00099'\n",
    "    >>> '{0:.6}'.format(round_sigfigs(0.00098765, sig_figs=3))\n",
    "    '0.000988'\n",
    "    \"\"\"\n",
    "    if num != 0:\n",
    "        return round(num, -int(math.floor(math.log10(abs(num))) - (sig_figs - 1)))\n",
    "    else:\n",
    "        return 0  # Can't take the log of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorizeCalModification(repository,commit,fileName):\n",
    "    \n",
    "    ### types of content modifications:\n",
    "    ### data-affecting: parameters added, parameters deleted, value changes, value truncations type 2 (3 sig figs)\n",
    "    ### non-data-affecting: serial number changes, value truncations type 1 (7 sig figs), notes changed, line endings changed\n",
    "    \n",
    "    from io import StringIO\n",
    "    \n",
    "    modification = []\n",
    "    parserError = 0\n",
    "    \n",
    "    git_command = ['git', 'show','--pretty=format:\"%H\"', commit + '^1']\n",
    "    \n",
    "    git_query = Popen(git_command, cwd=repository, stdout=PIPE, stderr=PIPE, encoding='utf-8', errors='ignore')\n",
    "    commitEntry = re.search(r'\"([a-zA-Z0-9]{0,40})\"',git_query.stdout.readline())\n",
    "    git_query.stdout.close()\n",
    "    if commitEntry:\n",
    "        previousCommit = commitEntry.group(1)\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    git_command_file1 = ['git','cat-file','-p',previousCommit + ':./' + fileName]\n",
    "    git_command_file2 = ['git','cat-file','-p',commit + ':./' + fileName]\n",
    "    git_query_file1 = Popen(git_command_file1, cwd=repository, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "    file1_read = git_query_file1.stdout.read()\n",
    "    git_query_file1.stdout.close()\n",
    "    file1 = StringIO(file1_read)\n",
    "    git_query_file2 = Popen(git_command_file2, cwd=repository, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "    file2_read = git_query_file2.stdout.read()\n",
    "    git_query_file2.stdout.close()\n",
    "    file2 = StringIO(file2_read)\n",
    "    \n",
    "    try:\n",
    "        df_file1 = pd.read_csv(file1, sep=\",\", converters = {'value': np.float64}, float_precision='round_trip')\n",
    "    except ValueError:\n",
    "        try:\n",
    "            file1 = StringIO(file1_read)\n",
    "            df_file1 = pd.read_csv(file1, sep=\",\", float_precision='round_trip')\n",
    "        except ValueError:\n",
    "            modification.append('parserError:PreviousValueFormatInvalid')\n",
    "            parserError = 1\n",
    "        \n",
    "    try:\n",
    "        df_file2 = pd.read_csv(file2, sep=\",\", converters = {'value': np.float64}, float_precision='round_trip')\n",
    "    except ValueError:\n",
    "        try:     \n",
    "            file2 = StringIO(file2_read)\n",
    "            df_file2 = pd.read_csv(file2, sep=\",\")\n",
    "        except ValueError:\n",
    "            modification.append('parserError:CurrentValueFormatInvalid')\n",
    "            parserError = 1\n",
    "        \n",
    "    if parserError == 1:\n",
    "        return set(modification)\n",
    "        \n",
    "    if fileName.endswith('.csv'):\n",
    "        \n",
    "        df_file1.set_index('name',inplace=True)\n",
    "        df_file2.set_index('name',inplace=True)\n",
    "    \n",
    "        parametersDeleted = []\n",
    "        parametersAdded = []\n",
    "        linesDeleted = df_file1[~df_file1.index.isin(df_file2.index)]\n",
    "        if not linesDeleted.empty:\n",
    "            parametersDeleted.append(list(linesDeleted.index.values))\n",
    "            modification.append('parametersDeleted')\n",
    "        linesAdded = df_file2[~df_file2.index.isin(df_file1.index)]\n",
    "        if not linesAdded.empty:\n",
    "            parametersAdded.append(list(linesAdded.index.values))\n",
    "            modification.append('parametersAdded')\n",
    "\n",
    "        paramsDeleted = [item for sublist in parametersDeleted for item in sublist]\n",
    "        paramsAdded = [item for sublist in parametersAdded for item in sublist]\n",
    "    \n",
    "        for calCoeff, row in df_file1.iterrows():\n",
    "            if calCoeff not in paramsDeleted and calCoeff not in paramsAdded:\n",
    "                cal1 = row['value']\n",
    "                cal2 = df_file2.loc[calCoeff,'value']\n",
    "                if isinstance(cal1, str) or isinstance(cal2, str):\n",
    "                    if isinstance(cal1, str):\n",
    "                        try:\n",
    "                            cal1 = float(cal1)\n",
    "                        except ValueError:\n",
    "                            if ',' in cal1:\n",
    "                                try:\n",
    "                                    cal1 = cal1.strip('[]').split(',')\n",
    "                                except ValueError:\n",
    "                                    modification.append('parserError:PreviousValueFormatInvalid')\n",
    "                    if isinstance(cal2, str):\n",
    "                        try:\n",
    "                            cal2 = float(cal2)\n",
    "                        except ValueError:\n",
    "                            if ',' in cal2:\n",
    "                                try:\n",
    "                                    cal2 = cal2.strip('[]').split(',')\n",
    "                                except ValueError:\n",
    "                                    modification.append('parserError:CurrentValueFormatInvalid')\n",
    "                if isinstance(cal1, float) and isinstance(cal2, float):\n",
    "                    if cal2 != cal1:\n",
    "                        if round_sigfigs(cal2,7) == cal1 or round_sigfigs(cal1,7) == cal2:\n",
    "                            modification.append('valueResolutionChanged_7sigfigs')\n",
    "                        elif round_sigfigs(cal2,3) == cal1 or round_sigfigs(cal1,3) == cal2:\n",
    "                            modification.append('valueResolutionChanged_3sigfigs')\n",
    "                        else:\n",
    "                            modification.append('valuesModified')\n",
    "                elif isinstance(cal1, str) and isinstance(cal2, str):\n",
    "                    if cal2 != cal1:\n",
    "                        modification.append('stringValuesModified')\n",
    "                elif isinstance(cal1, list) and isinstance(cal2, list):\n",
    "                    if cal2 != cal1:\n",
    "                        for x,y in zip(cal1,cal2):\n",
    "                            if x!= y:\n",
    "                                try:\n",
    "                                    x = float(x)\n",
    "                                    y = float(y)\n",
    "                                    if round_sigfigs(y,7) == x or round_sigfigs(x,7) == y:\n",
    "                                        modification.append('valueResolutionChanged_7sigfigs')\n",
    "                                    elif round_sigfigs(y,2) == x or round_sigfigs(x,2) == y:\n",
    "                                        modification.append('valueResolutionChanged_3sigfigs')\n",
    "                                    else:\n",
    "                                        modification.append('valuesModified')\n",
    "                                except ValueError:\n",
    "                                    modification.append('comparisonError:ValueFormatInvalid')\n",
    "                elif type(cal1) != (type(cal2)):\n",
    "                    modification.append('comparisonError:ValueFormatIncompatible') \n",
    "                    \n",
    "                sn1 = row['serial']\n",
    "                sn2 = df_file2.loc[calCoeff,'serial']\n",
    "                if isinstance(sn1, str) and isinstance(sn2, str):\n",
    "                    if sn1 != sn2:\n",
    "                        modification.append('serialNumberChanged')\n",
    "                else:\n",
    "                    modification.append('comparisonError:incompatibleSerialNumberFormats')\n",
    "                notes1 = row['notes']\n",
    "                notes2 = df_file2.loc[calCoeff,'notes']\n",
    "                if isinstance(notes1, str) and isinstance(notes2, str):\n",
    "                    if notes1 != notes2:\n",
    "                        modification.append('notesLineEndingsChanged')\n",
    "                else:\n",
    "                    modification.append('comparisonError:incompatibleNotesFormats')\n",
    "\n",
    "    elif fileName.endswith('.ext'):\n",
    "        if df_file1.equals(df_file2):\n",
    "            modification.append('formattingLineEndingsChanged')\n",
    "        else:\n",
    "            modification.append('valuesModified')\n",
    "            \n",
    "    return set(modification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploymentLookup(RefDes, lookupDate):\n",
    "    ### Look up deployment number and asset ID for a reference designator and date.\n",
    "    ### TODO: configure to handle over-lapping deployments and return all relevant assetIDs and deploymentNumbers\n",
    "    \n",
    "    assetID = 'NaN'\n",
    "    deploymentNumber = 'NaN'\n",
    "    for deploy in RefDes_dict_byRefDes[RefDes]:\n",
    "        if 'nan' in str(deploy['deployEnd']):\n",
    "            if deploy['deployDate'] < lookupDate:\n",
    "                assetID = deploy['AssetID']\n",
    "                deploymentNumber = deploy['deployment']\n",
    "        else:\n",
    "            deployEnd = datetime.datetime.strptime(str(deploy['deployEnd']), '%Y-%m-%dT%H:%M:%S')\n",
    "            if deploy['deployDate'] < lookupDate and deployEnd > lookupDate:\n",
    "                assetID = deploy['AssetID']\n",
    "                deploymentNumber = deploy['deployment']\n",
    "        \n",
    "    return(assetID, deploymentNumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the start of the main code block.  You may need to alter the regEx on line 14 to match your local directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read directory listing of github directory LOCALLY\n",
    "\n",
    "githubFileList = []\n",
    "\n",
    "for sensor in sensorList:\n",
    "    sensorDir = localRepo + 'calibration/' + sensor\n",
    "    fileList = os.listdir(sensorDir)\n",
    "    for csvFile in fileList:\n",
    "        if str.startswith(csvFile,filePrefix):\n",
    "            githubFileList.append(localRepo + 'calibration/' + sensor + '/' + csvFile)\n",
    "\n",
    "sensorCals = {}\n",
    "for githubFile in githubFileList:\n",
    "    fileBits = re.search(r\"/.*/.*/.*(/.*/.*/((.*)__(.*).csv))\",githubFile)\n",
    "    if fileBits:\n",
    "        if fileBits.group(3) not in sensorCals:\n",
    "            sensorCals[fileBits.group(3)] = {'calFile': []}\n",
    "        sensorCals[fileBits.group(3)]['calFile'].append([datetime.datetime.strptime(fileBits.group(4), '%Y%m%d'),fileBits.group(2),fileBits.group(1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load in github vocab list and convert to Reference Designator dictionary\n",
    "vocabList = pd.read_csv(localRepo + '/vocab/vocab.csv')\n",
    "vocab_dict = vocabList.set_index('Reference_Designator').T.to_dict('series')\n",
    "\n",
    "### load in github bulk asset records and create Asset ID dictionary for manufacturer's serial number\n",
    "sensorList = pd.read_csv(localRepo + '/bulk/sensor_bulk_load-AssetRecord.csv')\n",
    "assetID_dict = pd.Series(sensorList[\"Manufacturer's Serial No./Other Identifier\"].values, index=sensorList['ASSET_UID']).to_dict()\n",
    "\n",
    "### load in github deployment sheets, sort by Asset ID, load deployment info in Reference designator dictionary\n",
    "allFiles = glob.glob(localRepo + \"deployment/*.csv\")\n",
    "df_deploy = pd.concat([pd.read_csv(f, skip_blank_lines = True, comment='#') for f in allFiles], ignore_index = True, sort=True)\n",
    "df_deploy_sort = df_deploy.sort_values(by=[\"sensor.uid\",\"startDateTime\"],ascending=False)\n",
    "\n",
    "### Create dictionary indexed by Asset ID\n",
    "RefDes_dict = {}\n",
    "for i in df_deploy_sort['sensor.uid'].unique():\n",
    "    RefDes_dict[i] = [{'deployDate':datetime.datetime.strptime(df_deploy_sort['startDateTime'][j], '%Y-%m-%dT%H:%M:%S'), 'deployEnd': df_deploy_sort['stopDateTime'][j], 'RefDes':df_deploy_sort['Reference Designator'][j], 'deployment':df_deploy_sort['deploymentNumber'][j], 'firstRawFile':'none', 'rawSN':'-99999'} for j in df_deploy_sort[df_deploy_sort['sensor.uid']==i].index]\n",
    "\n",
    "### Create dictionary indexed by Reference Designator\n",
    "RefDes_dict_byRefDes = {}\n",
    "for i in df_deploy_sort['Reference Designator'].unique():\n",
    "    RefDes_dict_byRefDes[i] = [{'deployDate':datetime.datetime.strptime(df_deploy_sort['startDateTime'][j], '%Y-%m-%dT%H:%M:%S'), 'deployEnd':df_deploy_sort['stopDateTime'][j], 'AssetID':df_deploy_sort['sensor.uid'][j],  'deployment':df_deploy_sort['deploymentNumber'][j]} for j in df_deploy_sort[df_deploy_sort['Reference Designator']==i].index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Determine which cal files are associated with active deployments\n",
    "\n",
    "gitHubRootURL = 'https://github.com/ooi-integration/asset-management/blob/master'\n",
    "\n",
    "for key,values in RefDes_dict.items():\n",
    "    if key in sensorCals:\n",
    "        calHistory = sensorCals[key]\n",
    "        for deployment in RefDes_dict[key]:\n",
    "            calDateList = list(filter(lambda d: d[0] < deployment['deployDate'], calHistory['calFile']))\n",
    "            if calDateList:\n",
    "                deploymentCalFile = min(calDateList, key = lambda x: abs(x[0]-deployment['deployDate']))\n",
    "                if deploymentCalFile:\n",
    "                    deployment['calFile'] = deploymentCalFile[1]\n",
    "                    deployment['calFileURL'] = gitHubRootURL + deploymentCalFile[2]\n",
    "            else:\n",
    "                ### Print out sensor deployments with no valid cal file\n",
    "                print('no valid cal file?')\n",
    "                print(calHistory)\n",
    "                print(key + ' ' + str(deployment['deployDate']) + ' ' + str(deployment['deployEnd']) + ' ' + deployment['RefDes'])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to modify the regEx expressions on line 7 and 10 to match your local directory and filename structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Retrieve modification history for each cal file in the githubFileList\n",
    "\n",
    "fileModHistory = {}\n",
    "\n",
    "for calFile in githubFileList:\n",
    "    print(calFile)\n",
    "    regEx_calFile = r\"(\" + localDir + r\".*/)(.*/.*/)(.*\\..*)\"\n",
    "    fileBits = re.search(regEx_calFile, calFile)\n",
    "    if fileBits:\n",
    "        regEx_instrument = r\"(\" + filePrefix + r\"\\S+-\\d{5}-\\d{5})\"\n",
    "        instrument = re.search(regEx_instrument,fileBits.group(3))\n",
    "        if instrument:\n",
    "            if instrument.group(1) not in fileModHistory:\n",
    "                fileModHistory[instrument.group(1)] = {}\n",
    "            fileModHistory[instrument.group(1)][fileBits.group(3)] = gitHubMine(fileBits.group(1),fileBits.group(2),fileBits.group(3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Classify modifications and create annotation text\n",
    "\n",
    "with open('annotationMissing.txt', 'r') as file:\n",
    "    annotationMissing = file.read().replace('\\n','')\n",
    "file.close()  \n",
    "\n",
    "with open('annotationModify.txt', 'r') as file:\n",
    "    annotationModify = file.read().replace('\\n','')\n",
    "file.close() \n",
    "\n",
    "with open('annotationTruncated.txt', 'r') as file:\n",
    "    annotationTruncated = file.read().replace('\\n','')\n",
    "file.close() \n",
    "\n",
    "with open('annotationDownstream.txt', 'r') as file:\n",
    "    annotationDownstream = file.read().replace('\\n','')\n",
    "file.close()\n",
    "\n",
    "changeHistory=[]\n",
    "\n",
    "def annotate(refDes, deployment, changeDate, startDate, endDate, URL, changeType):\n",
    "    if 'coefficient' in changeType:\n",
    "        if 'resolution' in changeType:\n",
    "            annotationString = annotationTruncated\n",
    "        if 'added' in changeType or 'deleted' in changeType or 'modified' in changeType:\n",
    "            annotationString = annotationModify\n",
    "    elif 'file' in changeType or 'File' in changeType:\n",
    "        annotationString = annotationMissing\n",
    "    else:\n",
    "        annotationString = 'NaN'\n",
    "        \n",
    "    if 'NaN' not in annotationString:\n",
    "        annotation = annotationString.format(refDes, deployment, changeDate, deployment, startDate, endDate, URL)\n",
    "    else:\n",
    "        annotation = 'NaN'\n",
    "        \n",
    "    return annotation\n",
    "\n",
    "def annotateDownstream(downstreamSensor, upstreamSensor, changeDate, startDate, endDate, URL):\n",
    "    annotation = annotationDownstream.format(downstreamSensor, upstreamSensor, downstreamSensor, changeDate, downstreamSensor, startDate, endDate, upstreamSensor, URL)\n",
    "        \n",
    "    return annotation\n",
    "\n",
    "def logChange(changeType):\n",
    "    import subprocess\n",
    "    \n",
    "    Array = vocab_dict[deployment['RefDes']]['TOC_L1']\n",
    "    Platform = vocab_dict[deployment['RefDes']]['TOC_L2']\n",
    "    Node = vocab_dict[deployment['RefDes']]['TOC_L3']\n",
    "    Instrument = vocab_dict[deployment['RefDes']]['Instrument']\n",
    "    assetID = deployment['calFile'].split('__')[0]\n",
    "    sensorSN = assetID_dict[assetID]\n",
    "    annotation = annotate(deployment['RefDes'], deployment['deployment'], changeDate, deployment['deployDate'], dateRangeEnd, deployment['calFileURL'], changeType)\n",
    "    change = {'Array': Array, 'Platform': Platform, 'Node': Node, 'Instrument': Instrument, 'RefDes': deployment['RefDes'], 'Asset ID': assetID, 'Serial Number': sensorSN, 'deployment':deployment['deployment'], 'gitHub changeDate': changeDate, 'OOI changeDate': '?', 'file': deployment['calFile'], 'URL': deployment['calFileURL'], 'changeType': changeType, 'dateRangeStart': deployment['deployDate'], 'dateRangeEnd': dateRangeEnd, 'annotation': annotation}\n",
    "    changeHistory.append(change)\n",
    "    # If CTD, check for downstream sensors to log\n",
    "    if 'CTD' in deployment['RefDes']:\n",
    "        refDes = deployment['RefDes'].split('-')\n",
    "        platform = refDes[0]\n",
    "        node = refDes[1]\n",
    "        PD = [193, 194, 195]\n",
    "        affectedList = []\n",
    "        for x in range(len(PD)):\n",
    "            programCall = localDir + 'preload-database/affectedQuery.py ' + str(PD[x]) + ' ' + platform + ' ' + node\n",
    "            output = subprocess.check_output(programCall, shell=True)\n",
    "            outputList = output.decode('utf-8').split('\\n')\n",
    "            for y in range(len(outputList)):\n",
    "                affectedList.append(outputList[y].split(' ')[0])         \n",
    "        affectedList_filtered = [x for x in set(affectedList) if len(x) > 5 and 'CTD' not in x]\n",
    "        print(affectedList_filtered)\n",
    "        for z in range(len(affectedList_filtered)):\n",
    "            Array = vocab_dict[affectedList_filtered[z]]['TOC_L1']\n",
    "            Platform = vocab_dict[affectedList_filtered[z]]['TOC_L2']\n",
    "            Node = vocab_dict[affectedList_filtered[z]]['TOC_L3']\n",
    "            Instrument = vocab_dict[affectedList_filtered[z]]['Instrument']\n",
    "            (assetID, deploymentNumber) = deploymentLookup(affectedList_filtered[z], changeDate)\n",
    "            if 'NaN' not in str(assetID) and 'NaN' not in str(deploymentNumber):\n",
    "                sensorSN = assetID_dict[assetID]\n",
    "                annotation = annotateDownstream(affectedList_filtered[z], deployment['RefDes'], changeDate, deployment['deployDate'], dateRangeEnd, deployment['calFileURL'])\n",
    "                change = {'Array': Array, 'Platform': Platform, 'Node': Node, 'Instrument': Instrument, 'RefDes': affectedList_filtered[z], 'Asset ID': assetID, 'Serial Number': sensorSN, 'deployment':deploymentNumber, 'gitHub changeDate': changeDate, 'OOI changeDate': '?', 'file': deployment['calFile'], 'URL': deployment['calFileURL'], 'changeType': changeType, 'dateRangeStart': deployment['deployDate'], 'dateRangeEnd': dateRangeEnd, 'annotation': annotation}\n",
    "                changeHistory.append(change)\n",
    "    \n",
    "for key,values in RefDes_dict.items():\n",
    "    if key in RefDes_dict:\n",
    "        print(key)\n",
    "        for deployment in RefDes_dict[key]:\n",
    "            if 'calFile' in deployment:\n",
    "                print(deployment['calFile'])\n",
    "                print(fileModHistory[key][deployment['calFile']])\n",
    "                for merge in fileModHistory[key][deployment['calFile']]:\n",
    "                    # only changes that occur after a deployment has taken place have the potential to affect data\n",
    "                    if 'unknown' not in str(merge):\n",
    "                        changeDate = datetime.datetime.strptime(merge, '%Y%m%dT%H%M%S')\n",
    "                        if 'nan' in str(deployment['deployEnd']):\n",
    "                            deployEnd = datetime.datetime.now()\n",
    "                        else:\n",
    "                            deployEnd = datetime.datetime.strptime(deployment['deployEnd'], '%Y-%m-%dT%H:%M:%S')\n",
    "                        if changeDate > deployment['deployDate']:\n",
    "                            if changeDate < deployEnd:\n",
    "                                dateRangeEnd = changeDate\n",
    "                            elif changeDate > deployEnd:\n",
    "                                dateRangeEnd = deployEnd\n",
    "                            addList = []\n",
    "                            deleteList = []\n",
    "                            renameList = []\n",
    "                            modifyList = []\n",
    "                        \n",
    "                            for modification in fileModHistory[key][deployment['calFile']][merge]:                               \n",
    "                                if 'ADD' in fileModHistory[key][deployment['calFile']][merge][modification]['modType']:\n",
    "                                    addList.append(modification)\n",
    "                                if 'DELETE' in fileModHistory[key][deployment['calFile']][merge][modification]['modType']:\n",
    "                                    deleteList.append(modification)\n",
    "                                if 'RENAME' in fileModHistory[key][deployment['calFile']][merge][modification]['modType']:\n",
    "                                    renameList.append([modification, fileModHistory[key][deployment['calFile']][merge][modification]['fileName'], fileModHistory[key][deployment['calFile']][merge][modification]['oldFileName']] )\n",
    "                                if 'MODIFY' in fileModHistory[key][deployment['calFile']][merge][modification]['modType']:\n",
    "                                    modifyList.append([modification, fileModHistory[key][deployment['calFile']][merge][modification]['modCategory']])\n",
    "                                                 \n",
    "                                \n",
    "                            if len(addList) > 0:\n",
    "                                print('file was added that was missing:')        \n",
    "                                print(addList)\n",
    "                                logChange('Missing file added')\n",
    "                                # file was \"missing\" and affects data = YES\n",
    "                            if len(deleteList) > 0:\n",
    "                                print('file deleted:')\n",
    "                                print(deleteList)\n",
    "                                logChange('File deleted')\n",
    "                                # file was \"missing\" for a period of time and affects data = YES\n",
    "                            if len(renameList) > 0:\n",
    "                                print('file renamed:')\n",
    "                                if len(renameList) > 1:\n",
    "                                    finalRename = max(renameList, key=lambda x: x[0])\n",
    "                                    oldFile = finalRename[1]\n",
    "                                    newFile = finalRename[2]\n",
    "                                else:\n",
    "                                    oldFile = renameList[0][1]\n",
    "                                    newFile = renameList[0][2]\n",
    "                                print(oldFile + ' renamed to: ' + newFile)\n",
    "                                print(deployment['deployDate'])\n",
    "                                # break fileName into asset ID and date\n",
    "                                oldFileBits = re.search(r\"calibration/.*/(AT.*)__(\\d{8})\\.csv\",oldFile)\n",
    "                                newFileBits = re.search(r\"calibration/.*/(AT.*)__(\\d{8})\\.csv\",newFile)\n",
    "                                if oldFileBits and newFileBits:\n",
    "                                    # if AT# changed file was missing for new AT# and affects data = YES\n",
    "                                    if oldFileBits.group(1) not in newFileBits.group(1):\n",
    "                                        print('file name changed to new asset ID')\n",
    "                                        logChange('File renamed with new Asset ID')\n",
    "                                    # if filedate changed and new date > deploymentStart date affects data = YES    \n",
    "                                    if oldFileBits.group(2) not in newFileBits.group(2):\n",
    "                                        print('file date name changed')\n",
    "                                        if datetime.datetime.strptime(oldFileBits.group(2),'%Y%m%d') > deployment['deployDate'] or \\\n",
    "                                        datetime.datetime.strptime(newFileBits.group(2),'%Y%m%d') > deployment['deployDate']:\n",
    "                                            print('file date changed')  \n",
    "                                            logChange('File renamed with new calibration date')\n",
    "                                else:\n",
    "                                    logChange('File renamed, does not match format')\n",
    "                            if len(modifyList) > 0:\n",
    "                                print('file modified')                           \n",
    "                                for entry in modifyList:\n",
    "                                    if 'valuesModified' in entry[1] or 'stringValuesModified' in entry[1] or 'comparisonError' in entry[1] or 'parserError' in entry[1]:\n",
    "                                        logChange('calibration coefficients were modified')\n",
    "                                    elif 'parametersDeleted' in entry[1]:\n",
    "                                        logChange('calibration coefficients were deleted')\n",
    "                                    elif 'parametersAdded' in entry[1]:\n",
    "                                        logChange('calibration coefficients were added') \n",
    "                                    elif 'valueResolutionChanged_3sigfigs' in entry[1]:\n",
    "                                        logChange('calibration coefficient resolution was added')  \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output all data-affecting changes to csv file \n",
    "\n",
    "with open('gitHubCalibrationSheets_dataAffectingChanges_v3.csv', 'w', encoding='utf8', newline='') as output_file:\n",
    "    fc = csv.DictWriter(output_file, fieldnames=changeHistory[0].keys())\n",
    "    fc.writeheader()\n",
    "    fc.writerows(changeHistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output all modifications to csv\n",
    "\n",
    "with open('gitHubCalibrationSheets_allChanges_v3.csv','w') as f:          \n",
    "    for key,values in fileModHistory.items():\n",
    "        for calFile in fileModHistory[key]:\n",
    "            for merge in fileModHistory[key][calFile]:\n",
    "                for mod in fileModHistory[key][calFile][merge]:\n",
    "                    f.write(\"%s, %s, %s\\n\" % (key,merge,fileModHistory[key][calFile][merge][mod]))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
