{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Serial numbers are mined from the raw data archive and pre-deploy/\n",
    "### post-recovery images each year post-cruise.  Parameter files \n",
    "### 'rawFileSN.csv' and 'imageSN.csv' include the most recently acquired\n",
    "### serial numbers, but must be refreshed each year after the cruise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critical Metadata Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import packages\n",
    "\n",
    "import datetime\n",
    "from datetime import date\n",
    "import metadataFunctions as mf\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import subprocess\n",
    "from subprocess import Popen\n",
    "import glob\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### If running preliminary pre-cruise verification, use local \n",
    "### asset management repo, as local changes to be verified will\n",
    "### not yet be on the main oceanobseratories repo.  Set useLocal to 'yes', \n",
    "### and enter path to local am repo.\n",
    "###\n",
    "### For post-cruise verification, set useLocal to 'no' in order to verify\n",
    "### the amRepo that has been ingested into CI.\n",
    "###\n",
    "### \n",
    "\n",
    "useLocal = 'yes'\n",
    "amRepo_local = '/Users/joeduprey/repos/asset-management'\n",
    "calRepo_local = '/Users/joeduprey/repos/calibrationFiles'\n",
    "deployHistoryDir = path.dirname('/Users/joeduprey/repos/deployments/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define date to append to output file names\n",
    "today = date.today()\n",
    "runDate = today.strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define output file names\n",
    "calVerifyFile = 'reportOuts/calibrationVerification_' + runDate + '.csv'\n",
    "deployVerifyFile = 'reportOuts/deploymentVerification_' + runDate + '.csv'\n",
    "sensorBulkMatchFile = 'reportOuts/sensorBulkVerification_' + runDate + '.txt'\n",
    "missingCalFile = 'reportOuts/missingCalFiles_' + runDate + '.txt'\n",
    "refDesList = 'reportOuts/refDesList_' + runDate + '.txt'\n",
    "\n",
    "### Define input parameter files !!!!!!!!! CHANGES EACH YEAR <================\n",
    "RawInputFile = 'params/rawFileSN_20250902.csv'\n",
    "imageSNfile = 'params/imageSN_2025.csv'\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "instrumentFile = 'params/RCA-InstrumentList.csv' # same updated file each year\n",
    "\n",
    "\n",
    "### Define directory pathways\n",
    "\n",
    "### Repo URL to access directory listing\n",
    "amRepo_head = 'https://github.com/oceanobservatories/asset-management/tree/master'\n",
    "calRepo_head = 'https://github.com/OOI-CabledArray/calibrationFiles/tree/master'\n",
    "\n",
    "if 'yes' in useLocal:\n",
    "    amRepo = amRepo_local\n",
    "    calRepo = calRepo_local\n",
    "else:\n",
    "    ### Repo base URL to directly load files\n",
    "    amRepo = 'https://raw.githubusercontent.com/oceanobservatories/asset-management/master'\n",
    "    \n",
    "    ### Repo base URL to directly load files\n",
    "    calRepo = 'https://raw.githubusercontent.com/OOI-CabledArray/calibrationFiles/master'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function to load file list from Github repo of calibration file format\n",
    "### Repo URL is the main repo (as opposed to the raw file content base url)\n",
    "### Optional to search for a specific type of file extention (i.e. 'csv')\n",
    "### A file extension of \"*\" will return all files.\n",
    "### Current regex works for calibration directories\n",
    "\n",
    "def github_fileList(repo_url,fileExt):\n",
    "    if '*' in fileExt:\n",
    "        fileExt = '[a-z]*'\n",
    "    \n",
    "    fileRegEx = re.compile('{\"name\":\"(AT.{25}\\.' + fileExt + ')\",')\n",
    "    result = requests.get(repo_url).text\n",
    "    filenameList = re.findall(fileRegEx, result)\n",
    "    \n",
    "    return filenameList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load in RCA Instrument list\n",
    "assetList = pd.read_csv(instrumentFile)\n",
    "assetList['mfgSN'] = assetList['mfgSN'].str.split(', ')\n",
    "assetList['instrumentType'] = assetList['instrumentType'].str.split(',')\n",
    "\n",
    "# import calibration coefficient maps and constants\n",
    "CalCoefficientMap = pd.read_csv('params/coefficientMap.csv')\n",
    "CalCoeff_dict = CalCoefficientMap.set_index('github').transpose().to_dict('list')\n",
    "\n",
    "CalConstants = pd.read_csv('params/coefficientConstants.csv', sep=\",\", converters = {'value': np.float64}, float_precision='round_trip')       \n",
    "CalConstants_dict = {}\n",
    "for idx,values in CalConstants.iterrows():\n",
    "    if values.sensor not in CalConstants_dict:\n",
    "        CalConstants_dict[values.sensor]={}\n",
    "    CalConstants_dict[values.sensor][values.coeff] = values.constant\n",
    "\n",
    "# load in list of github calibration files that have gone through 2i-HITL checks\n",
    "HITLcal = pd.read_csv('2i_HITL/2i_HITL_calibrationVerification.csv')\n",
    "\n",
    "# load in list of github deployment instances that have gone through 2i-HITL checks\n",
    "HITLdeploy = pd.read_csv('2i_HITL/2i_HITL_deploymentVerification.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load in bulk asset records from OOI asset-management github repo\n",
    "sensorList = pd.read_csv(amRepo + '/bulk/sensor_bulk_load-AssetRecord.csv')\n",
    "platformList = pd.read_csv(amRepo + '/bulk/platform_bulk_load-AssetRecord.csv')\n",
    "cruiseList = pd.read_csv(amRepo + '/cruise/CruiseInformation.csv')\n",
    "\n",
    "### Load in Cabled Array Deployment sheets from OOI asset-management github repo\n",
    "CabledArray = pd.Series(['CE02SHBP','CE04OSBP','CE04OSPD','CE04OSPS','RS01SBPD','RS01SBPS',\n",
    "                        'RS01SLBS','RS01SUM1','RS03AXBS','RS03AXPD','RS03AXPS','RS03INT2',\n",
    "                        'RS03INT1','RS01SUM2','RS03CCAL','RS03ECAL','RS03ASHS'])\n",
    "\n",
    "deploymentSheets = []\n",
    "for array in CabledArray:\n",
    "    deployFilePath = amRepo + '/deployment/' + array + '_Deploy.csv' \n",
    "    deploymentSheets.append(deployFilePath)\n",
    "    \n",
    "df_deploy = pd.concat([pd.read_csv(f, skip_blank_lines = True, comment='#') for f in deploymentSheets], ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load calibration directory list for sensors with cal sheets from OOI asset-management github repo,\n",
    "### filtered for Cabled Array sensors (AssetID starts with 'AT')\n",
    "### Excluding HYDLFA, OBSBBA, OBSSPA\n",
    "githubFileList = []\n",
    "sensorsWithCals = ['CTDBPN','CTDBPO','CTDPFA','CTDPFB','DOFSTA','DOSTAD','FLCDRA','FLNTUA','FLORDD', \\\n",
    "              'HYDBBA','NUTNRA','OPTAAC','OPTAAD', 'PARADA','PCO2WA', \\\n",
    "               'PCO2WB','PHSENA','PHSEND','SPKIRA','THSPHA','TMPSFA','TRHPHA','VEL3DA','ZPLSCB']\n",
    "\n",
    "\n",
    "for sensor in sensorsWithCals:\n",
    "    sensorDir = amRepo + '/calibration/' + sensor\n",
    "    if 'yes' in useLocal:\n",
    "        if os.path.exists(sensorDir):\n",
    "            fileList = os.listdir(sensorDir)\n",
    "        else:\n",
    "            print('directory does not exist!')\n",
    "    else:\n",
    "        github_url = amRepo_head + '/calibration/' + sensor\n",
    "        fileList = github_fileList(github_url,'csv')\n",
    "    for calFile in fileList:\n",
    "        if str.startswith(calFile,'AT'):\n",
    "            githubFileList.append(sensorDir + '/' + calFile)\n",
    "\n",
    "### load original vendor calibration directory list from OOI-CabledArray github repo\n",
    "calRepoList = []\n",
    "calRepoFileList = []\n",
    "if 'yes' in useLocal:\n",
    "    calRepoList = glob.glob(calRepo + '/*/*')\n",
    "    for calFile in calRepoList:\n",
    "        calRepoFileList.append(calFile.split('/')[6].split('.')[0])\n",
    "else:\n",
    "    for sensor in sensorsWithCals:\n",
    "        sensorDir = calRepo + sensor\n",
    "        github_url = calRepo_head + '/' + sensor\n",
    "        fileList = github_fileList(github_url,'*')\n",
    "        for calFile in fileList:\n",
    "            if str.startswith(calFile,'AT'):\n",
    "                calRepoList.append(github_url + '/' + calFile)\n",
    "                calRepoFileList.append(calFile.split('.')[0])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create metadata dictionaries\n",
    "\n",
    "# Github Sensor bulk AssetID key to mfgSN\n",
    "assetID_dict = pd.Series(sensorList[\"Manufacturer's Serial No./Other Identifier\"].values, index=sensorList['ASSET_UID']).to_dict()\n",
    "\n",
    "# Deployment sheet Reference Designator key to startDate, AssetID, rawFile\n",
    "df_deploy_sort = df_deploy.sort_values(by=[\"Reference Designator\",\"startDateTime\"],ascending=False)\n",
    "\n",
    "RefDes_dict = {}\n",
    "for i in df_deploy_sort['Reference Designator'].unique():\n",
    "    RefDes_dict[i] = [{'deployDate':datetime.datetime.strptime(df_deploy_sort['startDateTime'][j], '%Y-%m-%dT%H:%M:%S'), 'deployEnd':df_deploy_sort['stopDateTime'][j], 'AssetID':df_deploy_sort['sensor.uid'][j], 'deployNum':df_deploy_sort['deploymentNumber'][j],'calFile': 'none', 'calFile_verify': 'none', 'firstRawFile':'undef', 'rawSN':'undef','rawFile_verify':'none','imageAssetID':'undef','image_verify':'none'} for j in df_deploy_sort[df_deploy_sort['Reference Designator']==i].index]\n",
    "\n",
    "# RCA AssetID key to mfgSN, instrumentType\n",
    "asset_dict_RCA = assetList.set_index('assetID').T.to_dict('series')\n",
    "\n",
    "# Github sensor cals AssetID key to calibration dates (extracted from fileNames)\n",
    "sensorCals = {}\n",
    "for githubFile in githubFileList:\n",
    "    fileBits = re.search(r\"/.*/.*/.*/.*/.*/((.*)__(.*).csv)\",githubFile)\n",
    "    if fileBits:\n",
    "        if fileBits.group(2) not in sensorCals:\n",
    "            sensorCals[fileBits.group(2)] = {'calFile': []}\n",
    "        sensorCals[fileBits.group(2)]['calFile'].append([datetime.datetime.strptime(fileBits.group(3), '%Y%m%d'),fileBits.group(1)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Verify sensor bulk by comparing AT# and vendor serial numbers between github sensorBulk and RCA Instrument List\n",
    "\n",
    "match = []\n",
    "matchFormat = []\n",
    "mismatch = []\n",
    "missingFromSensorBulk = []\n",
    "missingFromRCAlist = []\n",
    "\n",
    "#*# identify assetsIDs in sensorBulk missing from RCA Instrument List\n",
    "for key in assetID_dict:\n",
    "    if 'ATAPL' in key or 'ATOSU' in key:\n",
    "        if key not in asset_dict_RCA:\n",
    "            missingFromRCAlist.append(key)\n",
    "            \n",
    "#*# identify mismatched serial numbers between sensorBulk and RCA Instrument List\n",
    "for key,values in asset_dict_RCA.items():\n",
    "    if key in assetID_dict:\n",
    "        matchCategory=[]\n",
    "        SensorBulkSN = str(assetID_dict[key]).strip()\n",
    "        if 'nan' in SensorBulkSN:\n",
    "            matchCategory.append('0')\n",
    "        else:\n",
    "            for SN in asset_dict_RCA[key]['mfgSN']:\n",
    "                RCA_SN = SN.strip()\n",
    "                if RCA_SN == SensorBulkSN:\n",
    "                    matchCategory.append('2')\n",
    "                elif mf.partialMatch(RCA_SN, SensorBulkSN, 3):\n",
    "                    matchCategory.append('1')\n",
    "                else:\n",
    "                    matchCategory.append('0')                  \n",
    "        if '2' in matchCategory:\n",
    "            match.append([key,asset_dict_RCA[key]['mfgSN'],SensorBulkSN])\n",
    "        elif '1' in matchCategory:\n",
    "            matchFormat.append([key,asset_dict_RCA[key]['mfgSN'],SensorBulkSN])\n",
    "        else:\n",
    "            mismatch.append([key,asset_dict_RCA[key]['mfgSN'],SensorBulkSN])\n",
    "#*# identify assetIDs in RCA list that are missing from sensorBulk\n",
    "    else:\n",
    "        missingFromSensorBulk.append(key)\n",
    "        \n",
    "        \n",
    "with open(sensorBulkMatchFile,'w') as f:\n",
    "    f.write('match\\n')\n",
    "    for entry in match:\n",
    "        f.write('%s, %s, %s\\n' % (entry[0], entry[1], entry[2]))\n",
    "    f.write('matchFormat\\n')\n",
    "    for entry in matchFormat:\n",
    "        f.write('%s, %s, %s\\n' % (entry[0], entry[1], entry[2]))\n",
    "    f.write('mismatch\\n')\n",
    "    for entry in mismatch:\n",
    "        f.write('%s, %s, %s\\n' % (entry[0], entry[1], entry[2]))\n",
    "    f.write('missingFromSensorBulk\\n')\n",
    "    for entry in missingFromSensorBulk:\n",
    "        f.write('%s\\n' % entry)\n",
    "    f.write('missingFromRCAlist\\n')\n",
    "    for entry in missingFromRCAlist:\n",
    "        f.write('%s\\n' % entry)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Verify calibration files\n",
    "\n",
    "calVerify_dict = {}\n",
    "githubFileList_names = []\n",
    "#*# for each calibration file in github Repo:\n",
    "for githubFile in githubFileList:\n",
    "    #*# extract instrument type and filename from path\n",
    "    fileBits = re.search(r\"/.*/.*/.*/.*/(.*)/((.*__.*).\\S{3})\",githubFile)\n",
    "    if fileBits:\n",
    "        instrument = fileBits.group(1)\n",
    "        fileName = fileBits.group(2)\n",
    "        fileNameSub = fileBits.group(3)\n",
    "        githubFileList_names.append(fileNameSub)\n",
    "        calVerify_dict[fileName] = {}\n",
    "        calVerify_dict[fileName]['instrument'] = instrument\n",
    "        #*# identify if file has undergone 2i-HITL verification  \n",
    "        if fileName in HITLcal['githubFile'].tolist():\n",
    "            calVerify_dict[fileName]['HITLstatus'] = HITLcal[HITLcal.githubFile == fileName].Status.item()\n",
    "            calVerify_dict[fileName]['HITLnotes'] = HITLcal[HITLcal.githubFile == fileName].HITLnotes.item()\n",
    "        else:\n",
    "            calVerify_dict[fileName]['HITLstatus'] = 'NA'\n",
    "            calVerify_dict[fileName]['HITLnotes'] = ' '\n",
    "        #*# Verify gitHub cal file is in calRepoDrive list\n",
    "        if fileNameSub not in calRepoFileList:\n",
    "            calVerify_dict[fileName]['calRepo_check'] = 'NOMATCH'\n",
    "        else:\n",
    "            calVerify_dict[fileName]['calRepo_check'] = 'MATCH'  \n",
    "            \n",
    "        #*# Load in gitHub cal file\n",
    "        try:\n",
    "            githubCal = pd.read_csv(githubFile, sep=\",\", converters = {'value': np.float64}, float_precision='round_trip')\n",
    "            fileLoad = 'Success'\n",
    "            calVerify_dict[fileName]['fileParse'] = 'SUCCESS_TYPE1'\n",
    "        except ValueError:\n",
    "            try:\n",
    "                githubCal = pd.read_csv(githubFile, sep=\",\", float_precision='round_trip')\n",
    "                fileLoad = 'Success'\n",
    "                calVerify_dict[fileName]['fileParse'] = 'SUCCESS_TYPE2'\n",
    "            except ValueError:\n",
    "                fileLoad = 'Fail'\n",
    "                calVerify_dict[fileName]['fileParse'] = 'FAIL'\n",
    "                \n",
    "        if fileLoad == 'Success':\n",
    "            #*# verify serial number is identical for each line and maps back to AT# in filename\n",
    "            if 'serial' not in githubCal.columns:\n",
    "                calVerify_dict[fileName]['serialNumber'] = 'NOTFOUND_FILE'\n",
    "            else:\n",
    "                if len(np.unique(githubCal['serial'])) > 1:\n",
    "                    calVerify_dict[fileName]['serialNumber'] = 'MULTIPLE'\n",
    "                else:\n",
    "                    fileNameSNstring = re.search(r\"/(\\S{5}-\\w+-\\d{5})\", githubFile)\n",
    "                    if fileNameSNstring:\n",
    "                        fileNameSN = str(assetID_dict[fileNameSNstring.group(1)]).strip()\n",
    "                        if 'nan' in fileNameSN:\n",
    "                            calVerify_dict[fileName]['serialNumber'] = 'NOTFOUND_SENSORBULK'\n",
    "                        else:\n",
    "                            if str(np.unique(githubCal['serial'])[0]) not in str(fileNameSN):\n",
    "                                calVerify_dict[fileName]['serialNumber'] = 'MISMATCH_SENSORBULK'\n",
    "                            else:\n",
    "                                calVerify_dict[fileName]['serialNumber'] = 'MATCH_SENSORBULK'\n",
    "                    else:\n",
    "                        calVerify_dict[fileName]['serialNumber'] = 'PARSING_ERROR'\n",
    "            #*# identify any duplicate coefficient names, determine if values are identical \n",
    "            #*# (exception for OPTAA .ext cal sheets which have no column headers and are in a different format than \n",
    "            #*# other cal sheets)\n",
    "            if '.ext' not in githubFile:\n",
    "                duplicates = githubCal[githubCal.duplicated('name')]\n",
    "                if not duplicates.empty:\n",
    "                    for dup in duplicates['name']:\n",
    "                        #*# retrieve dateframe of coefficient duplicates and drop all identical rows...\n",
    "                        #*# if rows remain, coefficient values were not identical\n",
    "                        if (githubCal[githubCal['name']==dup].drop_duplicates(keep=False)).empty:\n",
    "                            calVerify_dict[fileName]['duplicateCoeff'] = 'DUPLICATES_IDENTICAL'\n",
    "                        else:\n",
    "                            calVerify_dict[fileName]['duplicateCoeff'] = 'DUPLICATES_NOTIDENTICAL'\n",
    "                else:\n",
    "                    calVerify_dict[fileName]['duplicateCoeff'] = 'NONE'\n",
    "            #*# for each parse-able calibration file compare coefficients between github and calRepoDrive\n",
    "            if calVerify_dict[fileName]['calRepo_check'] == 'NOMATCH':\n",
    "                calVerify_dict[fileName]['vendorMatch'] = 'NAN'\n",
    "            elif calVerify_dict[fileName]['calRepo_check'] == 'MATCH':\n",
    "                calRepoDriveFile = calRepo + '/' + fileNameSub\n",
    "                calCompare = mf.compareCalCoefficients(githubCal, calRepoDriveFile, CalCoeff_dict, CalConstants_dict)\n",
    "                if calCompare[0] == 'NAN':\n",
    "                    calVerify_dict[fileName]['vendorMatch'] = 'NOTCOMPARED'\n",
    "                else:\n",
    "                    calVerify_dict[fileName]['vendorMatch'] = calCompare\n",
    "    else:\n",
    "        print('invalid fileName format...')\n",
    "                \n",
    "calRepo_gitHub_missing = []\n",
    "\n",
    "#*# for each file in calRepo identify any missing files from github\n",
    "calRepo_gitHub_index = np.where(~np.isin(calRepoFileList,githubFileList_names))\n",
    "for i in range(len(calRepo_gitHub_index[0])):\n",
    "    calRepo_gitHub_missing.append(calRepoFileList[calRepo_gitHub_index[0][i]])\n",
    "    \n",
    "with open(missingCalFile,'w') as f:\n",
    "    f.write('calRepo_gitHub_missing\\n')\n",
    "    for entry in set(calRepo_gitHub_missing):\n",
    "        f.write('%s\\n' % (entry))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(calVerifyFile,'w') as f:\n",
    "    f.write('githubFile,instrument,calRepo_check,HITLstatus,HITLnotes,fileParse,serialNumber,duplicateCoeff,vendorMatch\\n')\n",
    "    for key,values in calVerify_dict.items():\n",
    "        if 'ext' not in key:\n",
    "            f.write(\"%s,%s,%s,%s,%s,%s,%s,%s,%s\\n\" % (key, calVerify_dict[key]['instrument'], calVerify_dict[key]['calRepo_check'], calVerify_dict[key]['HITLstatus'],calVerify_dict[key]['HITLnotes'],calVerify_dict[key]['fileParse'], calVerify_dict[key]['serialNumber'], calVerify_dict[key]['duplicateCoeff'], calVerify_dict[key]['vendorMatch']))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor does not exist in sensorBulk: \n",
      "Series([], dtype: object)\n",
      "Mooring ID does not exist in platformBulk: \n",
      "Series([], dtype: object)\n",
      "Cruise does not exist in cruiseList: \n",
      "Series([], dtype: object)\n",
      "Duplicate assetIDs within same deployment year AND same deployment number: \n",
      "2013\n",
      "Series([], dtype: object)\n",
      "2014\n",
      "499     ATAPL-58320-00002 RS01SBPD-DP01A-06-DOSTAD104 ...\n",
      "986     ATAPL-58320-00002 RS03AXPS-PC03A-4A-DOSTAD303 ...\n",
      "1189    ATAPL-58340-00003 RS03INT1-MJ03C-07-RASFLA301 ...\n",
      "1190    ATAPL-58340-00003 RS03INT1-MJ03C-07-D1000A301 ...\n",
      "dtype: object\n",
      "Series([], dtype: object)\n",
      "2015\n",
      "Series([], dtype: object)\n",
      "1196    ATAPL-58340-00003 RS03INT1-MJ03C-07-RASFLA301 ...\n",
      "1197    ATAPL-58340-00003 RS03INT1-MJ03C-07-D1000A301 ...\n",
      "dtype: object\n",
      "Series([], dtype: object)\n",
      "2016\n",
      "Series([], dtype: object)\n",
      "1203    ATAPL-58340-00003 RS03INT1-MJ03C-07-RASFLA301 ...\n",
      "1204    ATAPL-58340-00003 RS03INT1-MJ03C-07-D1000A301 ...\n",
      "dtype: object\n",
      "Series([], dtype: object)\n",
      "2017\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "1208    ATAPL-58340-00003 RS03INT1-MJ03C-07-RASFLA301 ...\n",
      "1209    ATAPL-58340-00003 RS03INT1-MJ03C-07-D1000A301 ...\n",
      "dtype: object\n",
      "Series([], dtype: object)\n",
      "2018\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "1214    ATAPL-58340-00001 RS03INT1-MJ03C-07-RASFLA301 ...\n",
      "1215    ATAPL-58340-00001 RS03INT1-MJ03C-07-D1000A301 ...\n",
      "dtype: object\n",
      "Series([], dtype: object)\n",
      "2019\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "1219    ATAPL-58340-00002 RS03INT1-MJ03C-07-RASFLA301 ...\n",
      "1220    ATAPL-58340-00002 RS03INT1-MJ03C-07-D1000A301 ...\n",
      "dtype: object\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "2020\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "2021\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "1228    ATAPL-58340-00001 RS03INT1-MJ03C-07-RASFLA301 ...\n",
      "1229    ATAPL-58340-00001 RS03INT1-MJ03C-07-D1000A301 ...\n",
      "dtype: object\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "2022\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "1231    ATAPL-58340-00002 RS03INT1-MJ03C-07-D1000A301 ...\n",
      "1233    ATAPL-58340-00002 RS03INT1-MJ03C-07-RASFLA301 ...\n",
      "dtype: object\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "2023\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "1236    ATAPL-58340-00001 RS03INT1-MJ03C-07-D1000A301 ...\n",
      "1238    ATAPL-58340-00001 RS03INT1-MJ03C-07-RASFLA301 ...\n",
      "dtype: object\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "2024\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "1241    ATAPL-58340-00002 RS03INT1-MJ03C-07-D1000A301 ...\n",
      "1243    ATAPL-58340-00002 RS03INT1-MJ03C-07-RASFLA301 ...\n",
      "dtype: object\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "2025\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "1246    ATAPL-58340-00003 RS03INT1-MJ03C-07-D1000A301 ...\n",
      "1248    ATAPL-58340-00003 RS03INT1-MJ03C-07-RASFLA301 ...\n",
      "dtype: object\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n",
      "Series([], dtype: object)\n"
     ]
    }
   ],
   "source": [
    "### Verify deployment sheets\n",
    "\n",
    "#*# check for valid parameters in deployment sheet:\n",
    "\n",
    "#*# does sensor ID exist in github sensorBulk?\n",
    "sensorID_check = np.where(~np.isin(df_deploy[\"sensor.uid\"],sensorList.ASSET_UID))\n",
    "print('Sensor does not exist in sensorBulk: ')\n",
    "print(df_deploy[\"sensor.uid\"][sensorID_check[0]] + ' ' + df_deploy[\"Reference Designator\"][sensorID_check[0]])\n",
    "\n",
    "#*# does mooring ID exist in github platformBulk?\n",
    "mooringID_check = np.where(~np.isin(df_deploy[\"mooring.uid\"],platformList.ASSET_UID))\n",
    "print('Mooring ID does not exist in platformBulk: ')\n",
    "print(df_deploy[\"mooring.uid\"][mooringID_check[0]] + ' ' + df_deploy[\"Reference Designator\"][mooringID_check[0]])\n",
    "\n",
    "#*# does cruise exist in github cruiseList?\n",
    "CUID_check = np.where(~np.isin(df_deploy.CUID_Deploy,cruiseList.CUID))\n",
    "print('Cruise does not exist in cruiseList: ')\n",
    "print(df_deploy.CUID_Deploy[CUID_check[0]] + ' ' + df_deploy[\"Reference Designator\"][CUID_check[0]])\n",
    "\n",
    "#*# check for duplicate assetIDs within same deployment year AND same deployment number\n",
    "print('Duplicate assetIDs within same deployment year AND same deployment number: ')\n",
    "dates = pd.to_datetime(df_deploy['startDateTime'])\n",
    "years = np.unique(dates.dt.year)\n",
    "for year in years:\n",
    "    print(year)\n",
    "    yearIndex = np.where(dates.dt.year == year)\n",
    "    sensor = df_deploy['sensor.uid'][yearIndex[0]]\n",
    "    refDes = df_deploy['Reference Designator'][yearIndex[0]]\n",
    "    deploy = df_deploy['deploymentNumber'][yearIndex[0]]\n",
    "    deployList = np.unique(deploy)\n",
    "    \n",
    "    for d in deployList:\n",
    "        subDeploymentIndex = np.where(deploy == d)\n",
    "        depSensor = sensor.iloc[subDeploymentIndex[0]]\n",
    "        depRef = refDes.iloc[subDeploymentIndex[0]]\n",
    "        depDeploy = deploy.iloc[subDeploymentIndex[0]]\n",
    "        idx = depSensor.duplicated(keep=False)\n",
    "        print(depSensor[idx] + ' ' + depRef[idx] + ' ' + str(depDeploy[idx]))\n",
    "        \n",
    "#*# for each deployment in github repo        \n",
    "for key,values in RefDes_dict.items():\n",
    "    for deploy in RefDes_dict[key]:\n",
    "        #*# load HITL notes\n",
    "        keyYearString = key + '.' + deploy['deployDate'].strftime('%Y') + '.' + str(deploy['deployNum'])\n",
    "        if keyYearString in HITLdeploy['referenceDesignatorYearDeployNum'].tolist():\n",
    "            deploy['HITLstatus'] = HITLdeploy[HITLdeploy.referenceDesignatorYearDeployNum == keyYearString].Status.item()\n",
    "            deploy['HITLnotes'] = HITLdeploy[HITLdeploy.referenceDesignatorYearDeployNum == keyYearString].HITLnotes.item()\n",
    "        else:\n",
    "            deploy['HITLstatus'] = 'NA'\n",
    "            deploy['HITLnotes'] = ''         \n",
    "        #*# assign calibration file\n",
    "        if deploy['AssetID'] in sensorCals:\n",
    "            calHistory = sensorCals[deploy['AssetID']]\n",
    "            calDateList = list(filter(lambda d: d[0] < deploy['deployDate'], calHistory['calFile']))\n",
    "            if calDateList:\n",
    "                deploymentCalFile = min(calDateList, key = lambda x: abs(x[0]-deploy['deployDate']))\n",
    "                #*# is calibration file available?\n",
    "                if deploymentCalFile:\n",
    "                    deploy['calFile'] = deploymentCalFile[1]\n",
    "                    #*# is caldate within 1 year of deployment?\n",
    "                    if deploy['deployDate'] - deploymentCalFile[0] > datetime.timedelta(days = 450):\n",
    "                        deploy['calFile_verify'] = 'VALID_FILE_CAL_OLDER_THAN_15MONTHS'\n",
    "                    else:\n",
    "                        deploy['calFile_verify'] = 'VALID_FILE'\n",
    "            else:\n",
    "                deploy['calFile'] = 'noValidCalFile'\n",
    "                deploy['calFile_verify'] = 'NO_VALID_FILE'\n",
    "        \n",
    "        \n",
    "#*# identify if serial number in raw (first or data) file and if so compare with assetID map in github sensorBulk        \n",
    "rawCheckSensors = ['CTD','SPK','NUT','PAR','FLOR','PREST','TMPSFA','OPTAA','ADCP','PAR']\n",
    "rawCheckSensors_DP = ['ENG000000','VEL3DA105','FLCDRA103','FLNTUA103','DOSTAD105',\n",
    "                      'VEL3DA103','FLCDRA102','FLNTUA102','DOSTAD104',\n",
    "                      'VEL3DA303','FLCDRA302','FLNTUA302','DOSTAD304']\n",
    "excludeNodes = []\n",
    "#*# exclude MARUM PI sensor for now since there is no raw data in the archive\n",
    "excludeSensors = ['CTDPFA110']\n",
    "\n",
    "#*# load rawFileSN.csv\n",
    "rawFileSN_list = pd.read_csv(RawInputFile)\n",
    "rawKeyList = []\n",
    "for key, values in RefDes_dict.items():\n",
    "    for deployment in RefDes_dict[key]:\n",
    "        try:\n",
    "            deployment['rawSN'] = rawFileSN_list.loc[(rawFileSN_list['referenceDesignator'] == key) & (rawFileSN_list['deployYear'] == int(deployment['deployDate'].strftime('%Y'))), 'rawSerialNumber'].iloc[0]\n",
    "            deployment['firstRawFile'] = rawFileSN_list.loc[(rawFileSN_list['referenceDesignator'] == key) & (rawFileSN_list['deployYear'] == int(deployment['deployDate'].strftime('%Y'))), 'rawFile'].iloc[0]\n",
    "        except IndexError:\n",
    "            if any(sensor in key[18:27] for sensor in rawCheckSensors) or any(sensor in key[18:27] for sensor in rawCheckSensors_DP):\n",
    "                if all(sensor not in key[18:27] for sensor in excludeSensors):\n",
    "                    if any(node_ex in key[9:14] for node_ex in excludeNodes):\n",
    "                        print(\"node exclusion for: \" + key)\n",
    "                        next\n",
    "                    else:\n",
    "                        print('no raw file listed for: ' + key + ' ' + deployment['deployDate'].strftime('%Y'))\n",
    "                        deployment['rawSN'] = '-99999'\n",
    "                        deployment['firstRawFile'] = 'none'\n",
    "                        rawKeyList.append(key)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,values in RefDes_dict.items():\n",
    "    for deployment in RefDes_dict[key]:\n",
    "        if 'none' in deployment['firstRawFile']:\n",
    "            if any(sensor in key[18:27] for sensor in rawCheckSensors) \\\n",
    "            and all(sensor not in key[18:27] for sensor in excludeSensors) \\\n",
    "            and all(node_ex not in key[9:14] for node_ex in excludeNodes):\n",
    "                deployment['rawFile_verify'] = 'NO_FILE'\n",
    "        elif 'undef' in deployment['firstRawFile']:\n",
    "            deployment['rawFile_verify'] = 'NAN'\n",
    "        else:\n",
    "            if '-99999' in str(deployment['rawSN']):\n",
    "                deployment['rawFile_verify'] = 'NO_SN'\n",
    "            else:\n",
    "                if str(deployment['rawSN']) in assetID_dict[deployment['AssetID']]:\n",
    "                    deployment['rawFile_verify'] = 'MATCH'\n",
    "                else:\n",
    "                    rawAT = 'unknown'\n",
    "                    for IDkey, IDvalue in assetID_dict.items():\n",
    "                        if deployment['AssetID'][0:11] in IDkey:\n",
    "                            if str(deployment['rawSN']) in IDvalue:\n",
    "                                rawAT = IDkey\n",
    "                    deployment['rawFile_verify'] = 'MISMATCH: raw: ' + str(deployment['rawSN']) + ':' + rawAT\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown asset ID: \n",
      "78452-00004\n",
      "unknown asset ID: \n",
      "00003\n",
      "unknown asset ID: \n",
      "58452-00006\n",
      "unknown asset ID: \n",
      "ATAPL-58694-00004\n",
      "image AssetID missing!\n"
     ]
    }
   ],
   "source": [
    "#*# load image assetIDs\n",
    "imageSN_list = pd.read_csv(imageSNfile)\n",
    "\n",
    "for idx,image in imageSN_list.iterrows():\n",
    "    if pd.isnull(image['imageSerialNumber']) and not pd.isnull(image['imageAssetID']):\n",
    "        print('image Serial Number missing!')\n",
    "    elif pd.isnull(image['imageAssetID']) and not pd.isnull(image['imageSerialNumber']):\n",
    "        print('image AssetID missing!')\n",
    "    # check for discrepancies between serial number and assetID\n",
    "    if not pd.isnull(image['imageSerialNumber']) and not pd.isnull(image['imageAssetID']):\n",
    "        if image['imageAssetID'] not in assetID_dict:\n",
    "            print('unknown asset ID: ')\n",
    "            print(image['imageAssetID'])\n",
    "        else:\n",
    "            imageSN = image['imageSerialNumber'].split(',')\n",
    "            assetSN = assetID_dict[image['imageAssetID']]\n",
    "            any_in = lambda imageSN, assetSN: any(i in assetSN for i in imageSN)\n",
    "            if not any_in:\n",
    "                print('no match')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! <============================ CHANGE INPUT BELOW AS NEEDED\n",
    "latest_fuzzy_date = \"20250904\" # The fuzzy match script may not have been run the same day as is set to runDate...\n",
    "# Here we read in the output of the fuzzy match notebook which has been curated. For example, multiple \n",
    "# potential assetID matches have been pared down to 1, or typos in SN or AssetID have been identified and corrected\n",
    "# etc...\n",
    "fuzzy_df = pd.read_csv(f\"reportOuts/fuzzyMatches_HITL_{latest_fuzzy_date}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_df[\"matching_asset_ids\"] = fuzzy_df[\"matching_asset_ids\"].str.strip(\"[]'\")\n",
    "fuzzy_df[\"matching_mfg_sn\"] = fuzzy_df[\"matching_mfg_sn\"].str.strip(\"[]'\")\n",
    "\n",
    "fuzzy_df = fuzzy_df[[\"referenceDesignator\", \"imageFile\", \"deployYear\", \"matching_mfg_sn\", \"matching_asset_ids\"]]\n",
    "fuzzy_df = fuzzy_df.rename(columns={\"matching_mfg_sn\":\"imageSerialNumber\",\"matching_asset_ids\":\"imageAssetID\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instrument types that are verifiable by manufacturer serial numbers extracted from raw files\n",
    "verifiable_by_SN = ['CTD','SPK','NUT','PAR','FLOR','PREST','TMPSFA','OPTAA','ADCP','PAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo which contains subdirectories for instrument types that require calibration\n",
    "calibration_repo_url = 'https://api.github.com/repos/oceanobservatories/asset-management/contents/calibration'\n",
    "\n",
    "response = requests.get(calibration_repo_url)\n",
    "data = response.json()\n",
    "\n",
    "# extract subdirectories\n",
    "instruments_requiring_calibration = [instrument['name'] for instrument in data if instrument['type'] == 'dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in RefDes_dict.items():\n",
    "    #print(key)\n",
    "    for deployment in RefDes_dict[key]:\n",
    "        if any(substring in key for substring in instruments_requiring_calibration):\n",
    "            deployment[\"calibrationRequired\"] = True\n",
    "        else:\n",
    "            deployment[\"calibrationRequired\"] = 'NAN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATAPL-58336-00002\n",
      "ATAPL-78452-00004\n",
      "ATAPL-58337-00009\n",
      "ATAPL-58694-00006\n",
      "ATAPL-58337-00001\n",
      "ATAPL-58320-00012\n",
      "ATAPL-78520-00002\n",
      "ATAPL-58336-00003\n",
      "ATAPL-58336-00003\n",
      "ATAPL-58341-00004\n",
      "ATAPL-58341-00004\n",
      "ATAPL-78452-00003\n",
      "ATAPL-78452-00003\n",
      "ATOSU-58332-00008\n",
      "ATOSU-58332-00008\n",
      "ATAPL-58337-00005\n",
      "ATAPL-58337-00005\n",
      "ATAPL-58694-00005\n",
      "ATAPL-58694-00005\n",
      "ATAPL-58337-00003\n",
      "ATAPL-58320-00011\n",
      "ATAPL-78520-00001\n",
      "ATOSU-58336-00005\n",
      "ATAPL-78452-00006\n",
      "ATOSU-58337-00011\n",
      "ATOSU-58694-00004\n",
      "ATAPL-58336-00009\n",
      "ATAPL-91990-00001\n",
      "ATAPL-58337-00010\n",
      "ATOSU-58320-00017\n",
      "ATOSU-70571-00003\n",
      "ATOSU-70570-00003\n",
      "ATOSU-58320-00022\n",
      "nan\n",
      "ATOSU-70570-00004\n",
      "ATOSU-58320-00019\n"
     ]
    }
   ],
   "source": [
    "# This loop looks up the image assetID that we chose from the output of fuzzy match as it loops through the deployment dictionary\n",
    "# It marks each instrument deployment image verification as eith MATCH, MISMATCH or NAN\n",
    "for key, values in RefDes_dict.items():\n",
    "    #print(key)\n",
    "    for deployment in RefDes_dict[key]:\n",
    "        try: #TODO important line below\n",
    "            deployment['imageAssetID'] = fuzzy_df.loc[(fuzzy_df['referenceDesignator'] == key) & (fuzzy_df['deployYear'] == int(deployment['deployDate'].strftime('%Y'))), 'imageAssetID'].iloc[0]\n",
    "            print(fuzzy_df.loc[(fuzzy_df['referenceDesignator'] == key) & (fuzzy_df['deployYear'] == int(deployment['deployDate'].strftime('%Y'))), 'imageAssetID'].iloc[0])\n",
    "            if str(deployment['imageAssetID']) in deployment['AssetID']:\n",
    "                deployment['image_verify'] = 'MATCH'\n",
    "            else:\n",
    "                deployment['image_verify'] = 'MISMATCH: image: ' + str(deployment['imageAssetID']) + ' deployment:' + str(deployment['AssetID'])\n",
    "        except IndexError:\n",
    "            deployment['image_verify'] = 'NAN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in RefDes_dict.items():\n",
    "    for deployment in RefDes_dict[key]:\n",
    "        if (deployment['rawFile_verify'] == 'MATCH') or (deployment['HITLstatus'] == 'Clear') or (deployment['image_verify'] == 'MATCH'):\n",
    "            deployment['verificationStatus'] = 'VERIFIED'\n",
    "        elif any(substring in key for substring in verifiable_by_SN):\n",
    "            deployment['verificationStatus'] = 'RAW_SN_POSSIBLE'\n",
    "        else:\n",
    "            deployment['verificationStatus'] = 'NOT_VERIFIED'\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(deployVerifyFile,'w') as f:\n",
    "    f.write('verificationStatus,referenceDesignator,deployYear,deploymentSheetAssetID,rawFileVerification,imageVerification,calFileVerification,calibrationRequired,HITLstatus,HITLnotes\\n')\n",
    "    for key,values in RefDes_dict.items():\n",
    "        for deployment in RefDes_dict[key]:\n",
    "            f.write(\"%s,%s,%s,%s,%s,%s,%s,%s,%s,%s\\n\" % (deployment['verificationStatus'],key, deployment['deployDate'].strftime('%Y'), deployment['AssetID'], deployment['rawFile_verify'], deployment['image_verify'], deployment['calFile_verify'], deployment['calibrationRequired'], deployment['HITLstatus'],deployment['HITLnotes']))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile deployment and calibration file listing by instrument type\n",
    "\n",
    "deployHistory_dict = {}\n",
    "for i in df_deploy_sort['Reference Designator'].unique():\n",
    "    deployHistory_dict[i] = [{'deployDate':datetime.datetime.strptime(df_deploy_sort['startDateTime'][j], '%Y-%m-%dT%H:%M:%S'), 'deployEnd':df_deploy_sort['stopDateTime'][j], 'AssetID':df_deploy_sort['sensor.uid'][j], 'deployNum':df_deploy_sort['deploymentNumber'][j],'vendorCalFile': 'none','githubCalFile': 'none','instrumentType': 'none','instrumentSN': 'none'} for j in df_deploy_sort[df_deploy_sort['Reference Designator']==i].index]\n",
    "\n",
    "# Github sensor cals AssetID key to calibration dates (extracted from fileNames)\n",
    "githubLink = amRepo_head + '/calibration/'\n",
    "githubSensorCals = {}\n",
    "\n",
    "for githubFile in githubFileList:\n",
    "    if 'yes' in useLocal:\n",
    "        fileLink = githubFile.replace(amRepo + '/calibration/',githubLink)\n",
    "    else:\n",
    "        fileLink = githubFile.replace(amRepo + '/calibration/',githubLink)\n",
    "    fileBits = re.search(r\"/.*/.*/.*/.*/.*/((.*)__(.*).csv)\",githubFile)\n",
    "    if fileBits:\n",
    "        if fileBits.group(2) not in githubSensorCals:\n",
    "            githubSensorCals[fileBits.group(2)] = {'calFile': []}\n",
    "        githubSensorCals[fileBits.group(2)]['calFile'].append([datetime.datetime.strptime(fileBits.group(3), '%Y%m%d'),fileLink])\n",
    "        \n",
    "# Vendor sensor cals AssetID key to calibration dates (extracted from fileNames)\n",
    "calRepoLink = calRepo_head + '/'\n",
    "vendorSensorCals = {}\n",
    "\n",
    "for vendorFile in calRepoList:\n",
    "    if 'yes' in useLocal:\n",
    "        fileLink = vendorFile.replace(calRepo + '/',calRepoLink)\n",
    "    else:\n",
    "        fileLink = vendorFile\n",
    "    fileBits = re.search(r\"/.*/.*/.*/.*/((.*)__([0-9]*).*\\..*$)\",vendorFile)\n",
    "\n",
    "    if fileBits:\n",
    "        if fileBits.group(2) not in vendorSensorCals:\n",
    "            vendorSensorCals[fileBits.group(2)] = {'calFile': []}\n",
    "        vendorSensorCals[fileBits.group(2)]['calFile'].append([datetime.datetime.strptime(fileBits.group(3), '%Y%m%d'),fileLink])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "instType = []\n",
    "\n",
    "for key,values in deployHistory_dict.items():\n",
    "    for deploy in deployHistory_dict[key]:  \n",
    "        #*# lookup instrumentType and instrumentSN\n",
    "        if deploy['AssetID'] in asset_dict_RCA:\n",
    "            deploy['instrumentType'] = '_'.join(asset_dict_RCA[deploy['AssetID']]['instrumentType']).replace('-','')\n",
    "            deploy['instrumentSN'] = asset_dict_RCA[deploy['AssetID']]['mfgSN']\n",
    "        else:\n",
    "            print('AssetID not in RCA Asset List')\n",
    "            print(deploy['AssetID'])\n",
    "            deploy['instrumentType'] = 'noValidType'\n",
    "            deploy['instrumentSN'] = ['noValidSN']\n",
    "        instType.append(deploy['instrumentType'])\n",
    "        #*# assign github calibration file\n",
    "        if deploy['AssetID'] in githubSensorCals:\n",
    "            calHistory = githubSensorCals[deploy['AssetID']]\n",
    "            calDateList = list(filter(lambda d: d[0] < deploy['deployDate'], calHistory['calFile']))\n",
    "            if calDateList:\n",
    "                deploymentCalFile = min(calDateList, key = lambda x: abs(x[0]-deploy['deployDate']))\n",
    "                #*# is calibration file available?\n",
    "                if deploymentCalFile:\n",
    "                    deploy['githubCalFile'] = deploymentCalFile[1]\n",
    "            else:\n",
    "                deploy['githubCalFile'] = 'noValidCalFile'\n",
    "                \n",
    "        #*# assign vendor calibration file\n",
    "        if deploy['AssetID'] in vendorSensorCals:\n",
    "            calHistory = vendorSensorCals[deploy['AssetID']]\n",
    "            calDateList = list(filter(lambda d: d[0] < deploy['deployDate'], calHistory['calFile']))\n",
    "            if calDateList:\n",
    "                ### TODO: add capability to list multiple files as vendor file...i.e. OPTAAC \".cal\" + \".dev\"\n",
    "                deploymentCalFile = min(calDateList, key = lambda x: abs(x[0]-deploy['deployDate']))\n",
    "                #*# is calibration file available?\n",
    "                if deploymentCalFile:\n",
    "                    deploy['vendorCalFile'] = deploymentCalFile[1]\n",
    "            else:\n",
    "                deploy['vendorCalFile'] = 'noValidCalFile'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "instTypes = set(instType)\n",
    "\n",
    "for inst in instTypes:\n",
    "    deploymentList = []\n",
    "    for key,values in deployHistory_dict.items():\n",
    "        for deployment in deployHistory_dict[key]:\n",
    "            if inst in deployment['instrumentType']:\n",
    "                deploymentList.append([deployment['instrumentType'],key,deployment['deployDate'],deployment['deployEnd'],deployment['AssetID'],deployment['instrumentSN'],deployment['githubCalFile'],deployment['vendorCalFile']])\n",
    "\n",
    "    deploymentList_sorted = sorted(deploymentList, key = lambda deploymentList: (deploymentList[1], deploymentList[2]))\n",
    "    deployHistoryFile = deployHistoryDir + '/' + inst + '_deployments.csv'    \n",
    "    with open(deployHistoryFile,'w') as f:\n",
    "        f.write('sensorType,referenceDesignator,startTime,endTime,assetID,instrumentSN,githubCalibrationFile,vendorCalibrationFile\\n')\n",
    "        for entry in deploymentList_sorted:\n",
    "            f.write(\"%s,%s,%s,%s,%s,\\\"%s\\\",%s,%s\\n\" % (entry[0],entry[1],entry[2],entry[3],entry[4],entry[5],entry[6],entry[7]))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyList = []\n",
    "for inst in instTypes:\n",
    "    for key,values in deployHistory_dict.items():\n",
    "        for deployment in deployHistory_dict[key]:\n",
    "            if inst in deployment['instrumentType']:\n",
    "                keyList.append(key)\n",
    "\n",
    "keyList_unique = set(keyList)\n",
    "\n",
    "#print(keyList_unique)\n",
    "with open(refDesList,'w') as f:\n",
    "    f.write('referenceDesignator\\n')\n",
    "    for entry in keyList_unique:\n",
    "        f.write(\"%s\\n\" % (entry))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
